---
name: phase-3-implement
description: Autonomous implementation loop with TDD, fresh agents per task, and self-healing
allowed-tools: [Read, Write, Bash, Task, Grep, Glob]
user-invocable: false
---

# Phase 3: Implementation Loop

## Overview | æ¦‚è¿°

Autonomously implement all tasks using TDD workflow, spawning fresh agents for each task, with automatic error recovery through Phase 4 healing.

ä½¿ç”¨ TDD å·¥ä½œæµè‡ªä¸»å®ç°æ‰€æœ‰ä»»åŠ¡ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆæ–°çš„ agentï¼Œé€šè¿‡ Phase 4 è‡ªæ„ˆè¿›è¡Œè‡ªåŠ¨é”™è¯¯æ¢å¤ã€‚

## When to Use | ä½•æ—¶ä½¿ç”¨

Invoked by autopilot-orchestrator as Phase 3, after Phase 2 (Breakdown) completes and user approves task plan.

## Input | è¾“å…¥

- Tasks directory: `.autopilot/tasks/`
- Task index: `.autopilot/tasks/index.json`
- Language config from index metadata

## Execution | æ‰§è¡Œ

### Step 1: Initialize Implementation Loop

**CRITICAL: Verify total task count BEFORE starting loop.**

```bash
# Get total task count from index
TOTAL_TASKS=$(autopilot-cli tasks list --json | jq 'length')

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸš€ Starting implementation of $TOTAL_TASKS tasks..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# Verify tasks exist
if [ "$TOTAL_TASKS" -eq 0 ]; then
  echo "âŒ ERROR: No tasks found in index!"
  echo "   Run Phase 2 (breakdown) first."
  exit 1
fi

# Initialize counters
COMPLETED_COUNT=0
FAILED_COUNT=0
HEALED_COUNT=0
LOOP_ITERATIONS=0
MAX_ITERATIONS=$((TOTAL_TASKS * 2))  # Safety: prevent infinite loops
```

### Step 2: Implementation Loop with Robust Verification

**CRITICAL: This loop MUST NOT exit until ALL tasks are accounted for.**

```bash
while true; do
  LOOP_ITERATIONS=$((LOOP_ITERATIONS + 1))

  # Safety check: prevent infinite loops
  if [ $LOOP_ITERATIONS -gt $MAX_ITERATIONS ]; then
    echo "âŒ CRITICAL ERROR: Loop exceeded $MAX_ITERATIONS iterations!"
    echo "   Expected $TOTAL_TASKS tasks, but processed $LOOP_ITERATIONS times."
    echo "   This indicates a bug in task management. Aborting."
    exit 1
  fi

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # STEP 1: Get next pending task
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  TASK_JSON=$(autopilot-cli tasks next --json 2>/dev/null)

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # STEP 2: Check if loop should exit
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  if [ -z "$TASK_JSON" ] || echo "$TASK_JSON" | jq -e '.error' > /dev/null; then
    # No more tasks from CLI - VERIFY before exiting

    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "ğŸ” No more pending tasks. Verifying completion..."
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

    # Count tasks by status
    PENDING_COUNT=$(autopilot-cli tasks list --status pending --json 2>/dev/null | jq 'length')
    IN_PROGRESS_COUNT=$(autopilot-cli tasks list --status in_progress --json 2>/dev/null | jq 'length')
    ACTUAL_COMPLETED=$(autopilot-cli tasks list --status completed --json 2>/dev/null | jq 'length')
    ACTUAL_FAILED=$(autopilot-cli tasks list --status failed --json 2>/dev/null | jq 'length')

    # Calculate totals
    ACCOUNTED_FOR=$((ACTUAL_COMPLETED + ACTUAL_FAILED))

    echo "ğŸ“Š Verification Results:"
    echo "   Total tasks:    $TOTAL_TASKS"
    echo "   Completed:      $ACTUAL_COMPLETED"
    echo "   Failed:         $ACTUAL_FAILED"
    echo "   Pending:        $PENDING_COUNT"
    echo "   In Progress:    $IN_PROGRESS_COUNT"
    echo "   Accounted for:  $ACCOUNTED_FOR"
    echo ""

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VERIFICATION: All tasks must be accounted for
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if [ $PENDING_COUNT -gt 0 ]; then
      echo "âŒ CRITICAL ERROR: Still have $PENDING_COUNT pending tasks!"
      echo "   But CLI returned no next task. This is a bug."
      echo ""
      echo "Pending tasks:"
      autopilot-cli tasks list --status pending
      echo ""
      echo "CANNOT proceed to Phase 5. Fix required."
      exit 1
    fi

    if [ $IN_PROGRESS_COUNT -gt 0 ]; then
      echo "âŒ CRITICAL ERROR: Still have $IN_PROGRESS_COUNT tasks in_progress!"
      echo "   These tasks are stuck. This is a bug."
      echo ""
      echo "In-progress tasks:"
      autopilot-cli tasks list --status in_progress
      echo ""
      echo "CANNOT proceed to Phase 5. Fix required."
      exit 1
    fi

    if [ $ACCOUNTED_FOR -ne $TOTAL_TASKS ]; then
      echo "âŒ CRITICAL ERROR: Task count mismatch!"
      echo "   Expected: $TOTAL_TASKS tasks"
      echo "   Got:      $ACCOUNTED_FOR tasks (completed + failed)"
      echo "   Missing:  $((TOTAL_TASKS - ACCOUNTED_FOR)) tasks"
      echo ""
      echo "CANNOT proceed to Phase 5. All tasks must be accounted for."
      exit 1
    fi

    # âœ… ALL CHECKS PASSED - Safe to exit
    echo "âœ… Verification passed! All $TOTAL_TASKS tasks accounted for."
    echo "   Completed: $ACTUAL_COMPLETED"
    echo "   Failed:    $ACTUAL_FAILED"
    echo ""
    break
  fi

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # STEP 3: Process next task
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  # Extract task details from enhanced next output
  TASK_ID=$(echo "$TASK_JSON" | jq -r '.task.id')
  TASK_DESC=$(echo "$TASK_JSON" | jq -r '.task.description')
  TASK_PRIORITY=$(echo "$TASK_JSON" | jq -r '.task.priority')
  TASK_EST_MIN=$(echo "$TASK_JSON" | jq -r '.task.estimatedMinutes')

  echo ""
  echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
  echo "ğŸ“ Task: $TASK_ID (Priority P$TASK_PRIORITY)"
  echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
  echo "ğŸ“‹ Description: $TASK_DESC"
  echo "â±ï¸  Estimated: ${TASK_EST_MIN} min"
  echo ""

  # Mark task as started
  autopilot-cli tasks start "$TASK_ID"

  # Record start time
  START_TIME=$(date +%s)

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # STEP 4: Implement task using Task tool
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # See Step 3 for actual implementation (spawns subagent)
  # This is where you call: Use Task tool with subagent_type="general-purpose"

  echo "ğŸ¤– Spawning implementer subagent..."

  # Build full implementer prompt from template in Step 3
  # Then invoke Task tool (see Step 3 instructions)

  # For now, this is a placeholder showing the flow
  # The actual Task tool invocation is documented in Step 3

  # Simulating task completion for pseudocode flow
  # In real execution, you'll capture result from Task tool
  IMPLEMENTATION_SUCCESS=true  # Replace with actual Task tool result

  # Record end time
  END_TIME=$(date +%s)
  DURATION=$((END_TIME - START_TIME))
  DURATION_MIN=$((DURATION / 60))
  DURATION_SEC=$((DURATION % 60))
  DURATION_STR="${DURATION_MIN}m ${DURATION_SEC}s"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # STEP 5: Handle result
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  if [ "$IMPLEMENTATION_SUCCESS" = true ]; then
    # Task succeeded
    COMPLETED_COUNT=$((COMPLETED_COUNT + 1))
    autopilot-cli tasks done "$TASK_ID" --duration "$DURATION_STR"

    echo "âœ… $TASK_ID completed"
    echo "   Duration: $DURATION_STR"
    echo ""
  else
    # Task failed - attempt auto-healing
    echo "âš ï¸  Implementation failed for $TASK_ID"
    echo ""

    echo "ğŸ”§ Attempting auto-heal..."
    # Invoke phase-4-heal (see Step 4 for details)
    # Use Skill tool to invoke "phase-4-heal"

    HEAL_SUCCESS=false  # Replace with actual heal result

    if [ "$HEAL_SUCCESS" = true ]; then
      # Healing succeeded
      COMPLETED_COUNT=$((COMPLETED_COUNT + 1))
      HEALED_COUNT=$((HEALED_COUNT + 1))
      autopilot-cli tasks done "$TASK_ID" --duration "$DURATION_STR (healed)"

      echo "âœ… Auto-healed successfully!"
      echo ""
    else
      # Healing failed - mark as failed and continue
      FAILED_COUNT=$((FAILED_COUNT + 1))
      autopilot-cli tasks fail "$TASK_ID" --reason "Implementation and healing failed"

      echo "âŒ Could not heal. Marked as failed."
      echo "   Will continue with remaining tasks."
      echo ""
    fi
  fi

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # STEP 6: Show progress
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  TOTAL_PROCESSED=$((COMPLETED_COUNT + FAILED_COUNT))
  PROGRESS_PCT=$((TOTAL_PROCESSED * 100 / TOTAL_TASKS))

  echo "ğŸ“Š Progress: $TOTAL_PROCESSED/$TOTAL_TASKS tasks ($PROGRESS_PCT%)"
  echo "   âœ… Completed: $COMPLETED_COUNT"
  echo "   âŒ Failed: $FAILED_COUNT"
  if [ $HEALED_COUNT -gt 0 ]; then
    echo "   ğŸ”§ Auto-healed: $HEALED_COUNT"
  fi
  echo ""

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # STEP 7: Safety check - verify we're making progress
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  if [ $TOTAL_PROCESSED -ge $TOTAL_TASKS ]; then
    echo "âœ… All $TOTAL_TASKS tasks processed. Exiting loop."
    break
  fi
done
```

**IMPORTANT NOTES:**

1. **Loop MUST NOT exit** until `completed + failed == total tasks`
2. **Verification is mandatory** before breaking loop
3. **If tasks are stuck** in pending/in_progress, loop will error (not silently fail)
4. **Infinite loop protection** prevents runaway execution
5. **Each task MUST use Task tool** to spawn subagent (see Step 3)

### Step 3: Implement Single Task

**For EACH task, you MUST spawn a fresh implementer agent using the Task tool.**

**CRITICAL:** Do NOT implement tasks yourself in the main session. ALWAYS use Task tool to spawn subagents.

**Implementation Flow:**

1. **Get task details from CLI:**
   ```bash
   TASK_JSON=$(autopilot-cli tasks next --json)
   TASK_ID=$(echo "$TASK_JSON" | jq -r '.id')
   ```

2. **Build implementer prompt:**
   Read the full task file content and create a comprehensive prompt that includes:
   - Task ID, description, module
   - Full acceptance criteria
   - Test requirements and patterns
   - Dependencies
   - TDD workflow instructions (see template below)

3. **Spawn fresh implementer subagent using Task tool:**

   **MANDATORY: Use the Task tool with these parameters:**

   ```
   Tool: Task
   Parameters:
     subagent_type: "general-purpose"
     description: "Implement task: {task.id}"
     prompt: "{full_implementer_prompt_from_template_below}"
     run_in_background: false  (blocking - wait for completion)
   ```

   **DO NOT skip this step. DO NOT implement in main session.**

4. **Wait for subagent to complete:**
   - The subagent will follow TDD workflow
   - Write tests first (RED phase)
   - Implement code (GREEN phase)
   - Refactor (REFACTOR phase)
   - Verify all tests pass
   - Report results

5. **Check subagent result:**
   - If successful â†’ Mark task as done using CLI
   - If failed â†’ Invoke Phase 4 (heal) for auto-recovery
   - Update progress and continue to next task

**Implementer Agent Prompt Template:**

Use this exact template when spawning implementer agents:

```markdown
You are an autonomous implementer agent for a single task in an autopilot workflow.

## TASK INFORMATION

**Task ID:** {task.id}
**Module:** {task.module}
**Priority:** P{task.priority}
**Estimated:** {task.estimatedMinutes} minutes

**Description:**
{task.description}

**Acceptance Criteria:**
{list each criterion with checkboxes}
- [ ] {criterion 1}
- [ ] {criterion 2}
...

**Dependencies:** {list or "None"}
**Test Pattern:** {task.testRequirements.unit.pattern}
**Test Required:** {task.testRequirements.unit.required}

---

## TDD IRON LAW (MANDATORY - NO EXCEPTIONS)

```
NO PRODUCTION CODE WITHOUT A FAILING TEST FIRST
```

**If you wrote code before the test â†’ DELETE it completely. Start over.**

This is non-negotiable. No "reference", no "adaptation", no "keeping it".
DELETE means DELETE.

## WORKFLOW (TDD)

### 1. RED Phase - Write Failing Tests

- Create test file: {test_pattern}
- Write ONE minimal test for FIRST acceptance criterion
- Run test â†’ MUST show failure (not error, actual test failure)
- Verify failure message is expected (feature missing, not typo)
- **If test passes immediately â†’ You're testing existing code, fix the test**
- **If test errors â†’ Fix error first, then verify it fails correctly**

### 2. GREEN Phase - Implement Minimum Code

- Write SMALLEST amount of code to pass the ONE test
- No extra features beyond what the test requires
- No premature optimization
- No refactoring of other code
- Run tests â†’ they MUST pass
- **If test still fails â†’ Fix code, NOT test**

### 3. REFACTOR Phase - Clean Code (ONLY after GREEN)

- Improve naming, structure
- Remove duplication
- Apply design patterns if appropriate
- Run tests after EACH change â†’ must stay passing
- **If tests fail during refactor â†’ Revert change immediately**

### 4. REPEAT - Next Test

- Go back to RED for next acceptance criterion
- One test at a time, one feature at a time
- Complete RED-GREEN-REFACTOR cycle for each

### 5. VERIFY - Final Check

Before marking complete, verify:
- [ ] All tests pass âœ“
- [ ] Coverage >80% âœ“
- [ ] All acceptance criteria satisfied âœ“
- [ ] No linting errors âœ“
- [ ] Output pristine (no warnings, no console.log) âœ“

## VERIFICATION CHECKLIST (MANDATORY)

Before marking task complete, verify EVERY item:

- [ ] Every new function/method has a test
- [ ] Watched each test fail BEFORE implementing (saw RED)
- [ ] Each test failed for expected reason (feature missing, not typo)
- [ ] Wrote minimal code to pass each test (no over-engineering)
- [ ] All tests pass (run full test suite)
- [ ] No test was written after the code (all tests-first)
- [ ] Tests use real code, not mocks (unless unavoidable)
- [ ] Edge cases and errors covered
- [ ] No production code exists without a corresponding test

**Cannot check all boxes? â†’ Start over with TDD. No exceptions.**

## RED FLAGS - STOP IMMEDIATELY

If you catch yourself:
- Writing code before test
- Test passes on first run (didn't see it fail)
- Adding multiple tests before any implementation
- Implementing features not required by current test
- "I'll add tests after to verify it works"
- "This is too simple to test"
- "I already manually tested it"
- Keeping code written before tests "as reference"

**ALL of these mean: DELETE CODE. Start over with TDD.**

## CONSTRAINTS

- Only implement what's specified in acceptance criteria
- Follow TDD Iron Law strictly (tests ALWAYS first, no exceptions)
- Use project's language/framework conventions
- Keep code clean and simple
- One test â†’ One minimal implementation â†’ Refactor â†’ Repeat

## OUTPUT REQUIREMENTS

When done, report:
1. **Files created/modified** (list all)
2. **Test results** (X/Y tests passed, Z% coverage)
3. **Duration** (actual time spent)
4. **Issues encountered** (if any)
5. **All acceptance criteria met** (confirm with checkboxes)

**CRITICAL:** Do NOT exit until ALL acceptance criteria are satisfied and ALL tests pass.

Start with RED phase now.
```

### Step 4: Invoke Healing (When Implementer Fails)

**When a task implementation fails, invoke Phase 4 (Heal) for automatic error recovery.**

**Auto-Healing Flow:**

1. **Capture error details:**
   - Extract error message from implementer agent output
   - Identify error type (dependency missing, syntax error, test failure, etc.)
   - Note which acceptance criteria failed

2. **Invoke phase-4-heal skill:**

   **Use Skill tool to invoke healing:**

   ```
   Tool: Skill
   Parameters:
     skill: "phase-4-heal"
     args: "--task-id {task.id} --error '{error_message}'"
   ```

   The heal skill will:
   - WebSearch for error solution
   - Apply fix automatically
   - Re-run tests
   - Return success/failure

3. **Check healing result:**
   - If healed successfully (tests pass):
     * Mark task as completed
     * Increment auto-healed counter
     * Log to debug.log
     * Continue to next task

   - If healing failed (still errors after 3 attempts):
     * Mark task as failed
     * Log detailed error to debug.log
     * Continue to next task (don't block entire workflow)

4. **Update progress:**
   ```bash
   echo "ğŸ”§ Auto-healed: $HEALED_COUNT errors"
   ```

**IMPORTANT:** Auto-healing should be attempted ONCE per task failure. If healing fails, mark task as failed and move on. Do NOT retry indefinitely.

### Step 5: Final Summary

```bash
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ¯ Implementation Complete!"
echo ""
echo "ğŸ“Š Final Statistics:"
echo "   Total tasks: $TASK_COUNT"
echo "   âœ… Completed: $COMPLETED"
echo "   âŒ Failed: $FAILED"
echo "   ğŸ”§ Auto-healed: $HEALED"
echo "   Success rate: $((COMPLETED * 100 / TASK_COUNT))%"
echo ""

if [ $FAILED -gt 0 ]; then
  echo "âš ï¸  Some tasks failed. They are marked with status='failed'."
  echo "   Review failed tasks in .autopilot/tasks/"
  echo ""
fi

echo "â–¶ï¸  Next: Phase 5 (Deliver) - Quality gates and PR creation"
```

### Step 6: Update State

```bash
# Update state to deliver phase
autopilot-cli state update --phase deliver
```

### Step 7: Return Result

```yaml
---PHASE RESULT---
phase: implement
status: complete
completed: {N}
failed: {M}
auto_healed: {K}
success_rate: {X}%
next_phase: deliver
summary: |
  Implemented {N}/{total} tasks successfully.
  Auto-healed {K} errors.
  {M} tasks failed and need manual attention.
  Ready for quality verification and delivery.
---END PHASE RESULT---
```

## Progress Updates | è¿›åº¦æ›´æ–°

Show progress after each task:

```
âœ… auth.signup.ui completed (3/35)
   Duration: 4m 32s
   Tests: 8/8 passed âœ“
   Coverage: 87%
   Files:
     - src/components/SignupForm.tsx (new)
     - tests/components/SignupForm.test.tsx (new)
   Next: auth.signup.validation
```

For errors with healing:

```
âš ï¸  Error in auth.signup.api
    Module 'bcrypt' not found
ğŸ”§ Auto-healing...
   Step 1: WebSearch "npm bcrypt install"
   Step 2: npm install bcrypt@5.1.0
   Step 3: Verify - npm test (âœ… 24/24 passed)
âœ… Healed successfully (1m 12s)
```

## Implementer Agent Template | å®ç°è€… Agent æ¨¡æ¿

When spawning implementer agents, use this prompt template:

```markdown
You are an autonomous implementer agent for a single task.

TASK: {task.id}
DESCRIPTION: {task.description}

ACCEPTANCE CRITERIA:
{criteria-list}

TEST REQUIREMENTS:
- Pattern: {test-pattern}
- Required: {yes/no}
- Min coverage: 80%

## TDD IRON LAW (MANDATORY - NO EXCEPTIONS)

```
NO PRODUCTION CODE WITHOUT A FAILING TEST FIRST
```

**If you wrote code before the test â†’ DELETE it completely. Start over.**

This is non-negotiable. No "reference", no "adaptation", no "keeping it".
DELETE means DELETE.

WORKFLOW (TDD):

1. RED Phase - Write Failing Tests:
   - Create test file: {test-pattern}
   - Write ONE minimal test for FIRST acceptance criterion
   - Run test â†’ MUST show failure (not error, actual test failure)
   - Verify failure message is expected (feature missing, not typo)
   - **If test passes immediately â†’ You're testing existing code, fix the test**
   - **If test errors â†’ Fix error first, then verify it fails correctly**

2. GREEN Phase - Implement Minimum Code:
   - Write SMALLEST amount of code to pass the ONE test
   - No extra features beyond what the test requires
   - No premature optimization
   - No refactoring of other code
   - Run tests â†’ they MUST pass
   - **If test still fails â†’ Fix code, NOT test**

3. REFACTOR Phase - Clean Code (ONLY after GREEN):
   - Improve naming, structure
   - Remove duplication
   - Apply design patterns if appropriate
   - Run tests after EACH change â†’ must stay passing
   - **If tests fail during refactor â†’ Revert change immediately**

4. REPEAT - Next Test:
   - Go back to RED for next acceptance criterion
   - One test at a time, one feature at a time
   - Complete RED-GREEN-REFACTOR cycle for each

5. VERIFY - Final Check:
   - All tests pass âœ“
   - Coverage >80% âœ“
   - All acceptance criteria satisfied âœ“
   - No linting errors âœ“
   - Output pristine (no warnings, no console.log) âœ“

## VERIFICATION CHECKLIST (MANDATORY)

Before marking task complete, verify EVERY item:

- [ ] Every new function/method has a test
- [ ] Watched each test fail BEFORE implementing (saw RED)
- [ ] Each test failed for expected reason (feature missing, not typo)
- [ ] Wrote minimal code to pass each test (no over-engineering)
- [ ] All tests pass (run full test suite)
- [ ] No test was written after the code (all tests-first)
- [ ] Tests use real code, not mocks (unless unavoidable)
- [ ] Edge cases and errors covered
- [ ] No production code exists without a corresponding test

**Cannot check all boxes? â†’ Start over with TDD. No exceptions.**

## RED FLAGS - STOP IMMEDIATELY

If you catch yourself:
- Writing code before test
- Test passes on first run (didn't see it fail)
- Adding multiple tests before any implementation
- Implementing features not required by current test
- "I'll add tests after to verify it works"
- "This is too simple to test"
- "I already manually tested it"
- Keeping code written before tests "as reference"

**ALL of these mean: DELETE CODE. Start over with TDD.**

CONSTRAINTS:
- Only implement what's specified in acceptance criteria
- Follow TDD Iron Law strictly (tests ALWAYS first, no exceptions)
- Use project's language/framework conventions
- Keep code clean and simple
- One test â†’ One minimal implementation â†’ Refactor â†’ Repeat

OUTPUT:
When done, report:
- Files created/modified
- Test results (pass count, coverage %)
- Duration
- Any issues encountered
```

## Error Handling | é”™è¯¯å¤„ç†

| Error | Action |
|-------|--------|
| No tasks found | Prompt to run Phase 2 |
| Task implementation fails | Invoke Phase 4 (heal) |
| Healing fails (max 3 attempts) | Mark task as failed, continue |
| All tasks fail | Continue to Phase 5, report failures |
| User interrupts | Save state, allow resume |

## Example Flow | ç¤ºä¾‹æµç¨‹

```
ğŸš€ Starting implementation of 35 tasks...

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ Task: setup.scaffold (Priority 1)
ğŸ“‹ Description: Initialize project structure
â±ï¸  Estimated: 15min

ğŸ¤– Spawning implementer agent...
âœ… setup.scaffold completed (1/35)
   Duration: 3m 47s

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ Task: auth.signup.ui (Priority 5)
ğŸ“‹ Description: Create signup form component
â±ï¸  Estimated: 20min

ğŸ¤– Spawning implementer agent...
âš ï¸  Error: Module 'react-hook-form' not found
ğŸ”§ Invoking auto-heal...
âœ… Healed successfully! (1m 23s)
âœ… auth.signup.ui completed (5/35)
   Duration: 6m 12s (healed)

ğŸ“Š Progress: 5/35 tasks (14%)
   âœ… Completed: 5
   âŒ Failed: 0
   ğŸ”§ Auto-healed: 1

[... continues for all 35 tasks ...]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ Implementation Complete!

ğŸ“Š Final Statistics:
   Total tasks: 35
   âœ… Completed: 33
   âŒ Failed: 2
   ğŸ”§ Auto-healed: 4
   Success rate: 94%

â–¶ï¸  Next: Phase 5 (Deliver)
```

## Rules | è§„åˆ™

1. **Fresh agent per task** - Prevent context pollution
2. **TDD workflow mandatory** - Tests before implementation
3. **Auto-heal on first error** - Try to fix automatically
4. **Mark failed after heal attempts** - Continue to next task
5. **Show progress regularly** - After each task and every 3 tasks
6. **Never skip tasks** - Process in priority order
7. **Save state continuously** - Can resume anytime

## Notes | æ³¨æ„äº‹é¡¹

- Implementation quality depends on clear acceptance criteria
- Fresh agents ensure no context bleed between tasks
- Auto-healing uses Phase 4 (WebSearch-powered)
- Failed tasks can be manually fixed and re-run later
- Progress is persisted in task files and state.json
